<snippet>
  <content><![CDATA[
# import libraries
import csv
#import urllib
#import urllib2
import re
import types
import time, datetime
import mechanize
import cookielib
from BeautifulSoup import BeautifulSoup, Tag, BeautifulStoneSoup
from dateutil import parser

# new instance of mechanize browser
mech = mechanize.Browser()

# Cookie Jar
cj = cookielib.LWPCookieJar()
mech.set_cookiejar(cj)

# mechanize browser options
mech.set_handle_equiv(True)
#mech.set_handle_gzip(True)
mech.set_handle_redirect(True)
mech.set_handle_referer(True)

# tell mechanize browser to ignore robots.txt
mech.set_handle_robots(False)

# follows refresh 0 but not hangs on refresh > 0
mech.set_handle_refresh(mechanize._http.HTTPRefreshProcessor(), max_time=1)

# adds debugging messages to ouput
mech.set_debug_http(True)
mech.set_debug_redirects(True)
mech.set_debug_responses(True)

# adds user-Agent
mech.addheaders = [('User-agent', 'Mozilla/5.0 (Windows; U; Windows NT 6.0; en-US) AppleWebKit/525.19 (KHTML, like Gecko) Chrome/1.0.154.53 Safari/525.19')]

# add URL to scrape here
prefixUrl = '${1}'

urlScrape = prefixUrl
pageScrape = mech.open(urlScrape)
htmlScrape = pageScrape.read()
soupScrape = BeautifulSoup(htmlScrape, convertEntities=BeautifulSoup.HTML_ENTITIES)

# print end status
print soupScrape
print ('Completed at %s' % datetime.datetime.now().strftime('%b %d, %Y at %I:%M %p'))
]]></content>
  <tabTrigger>soupscrape</tabTrigger>
  <description>Beautiful Soup Scraper</description>
  <!-- <scope>[add here]</scope> -->
</snippet>
